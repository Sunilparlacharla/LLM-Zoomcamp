{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2cd973",
   "metadata": {},
   "source": [
    "# If you're not running in Saturn Cloud, you need to install these libraries:\n",
    "\n",
    "Make sure you use the latest versions\n",
    "\n",
    "pip install -U transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c93493e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:12:34.166072Z",
     "iopub.status.busy": "2024-07-09T08:12:34.165681Z",
     "iopub.status.idle": "2024-07-09T08:12:34.169782Z",
     "shell.execute_reply": "2024-07-09T08:12:34.168966Z",
     "shell.execute_reply.started": "2024-07-09T08:12:34.166045Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ba46054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:12:35.062350Z",
     "iopub.status.busy": "2024-07-09T08:12:35.061960Z",
     "iopub.status.idle": "2024-07-09T08:12:35.727661Z",
     "shell.execute_reply": "2024-07-09T08:12:35.726777Z",
     "shell.execute_reply.started": "2024-07-09T08:12:35.062324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "overlay         100G   47G   54G  47% /\n",
      "tmpfs            64M     0   64M   0% /dev\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  100G   47G   54G  47% /run\n",
      "tmpfs            14G  4.0K   14G   1% /dev/shm\n",
      "/dev/nvme2n1    2.0G  1.6M  1.9G   1% /home/jovyan\n",
      "tmpfs            14G  120K   14G   1% /home/jovyan/.saturn\n",
      "tmpfs            14G   12K   14G   1% /run/secrets/kubernetes.io/serviceaccount\n",
      "tmpfs           7.7G   12K  7.7G   1% /proc/driver/nvidia\n",
      "tmpfs           7.7G  4.5M  7.7G   1% /run/nvidia-persistenced/socket\n",
      "tmpfs           7.7G     0  7.7G   0% /proc/acpi\n",
      "tmpfs           7.7G     0  7.7G   0% /sys/firmware\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c38e247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:12:55.741200Z",
     "iopub.status.busy": "2024-07-09T08:12:55.740790Z",
     "iopub.status.idle": "2024-07-09T08:12:55.745227Z",
     "shell.execute_reply": "2024-07-09T08:12:55.744384Z",
     "shell.execute_reply.started": "2024-07-09T08:12:55.741172Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/run/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c71d696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:12:58.196794Z",
     "iopub.status.busy": "2024-07-09T08:12:58.196381Z",
     "iopub.status.idle": "2024-07-09T08:12:59.684504Z",
     "shell.execute_reply": "2024-07-09T08:12:59.683612Z",
     "shell.execute_reply.started": "2024-07-09T08:12:58.196765Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9835c1080414bdd9dc22d84a5cb5535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer downloaded and saved to cache directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Set the environment variable to use the desired cache directory\n",
    "cache_dir = \"/run/cache/transformers\"\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "\n",
    "# Function to check if the model and tokenizer are already downloaded\n",
    "def is_model_and_tokenizer_cached(cache_dir):\n",
    "    model_files = [\n",
    "        \"pytorch_model.bin\", \n",
    "        \"config.json\", \n",
    "        \"tokenizer_config.json\", \n",
    "        \"spiece.model\"\n",
    "    ]\n",
    "    return all(os.path.exists(os.path.join(cache_dir, \"google/flan-t5-xl\", file)) for file in model_files)\n",
    "\n",
    "# Function to download the model and tokenizer if not cached\n",
    "def get_tokenizer_and_model():\n",
    "    if not is_model_and_tokenizer_cached(cache_dir):\n",
    "        # Download the tokenizer and model, caching them to the specified directory\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\", cache_dir=cache_dir)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", cache_dir=cache_dir, device_map=\"auto\")\n",
    "        print(\"Model and tokenizer downloaded and saved to cache directory.\")\n",
    "    else:\n",
    "        # Load the tokenizer and model from the cache directory\n",
    "        tokenizer = T5Tokenizer.from_pretrained(os.path.join(cache_dir, \"google/flan-t5-xl\"))\n",
    "        model = T5ForConditionalGeneration.from_pretrained(os.path.join(cache_dir, \"google/flan-t5-xl\"), device_map=\"auto\")\n",
    "        print(\"Model and tokenizer loaded from cache directory.\")\n",
    "    return tokenizer, model\n",
    "\n",
    "# Get the tokenizer and model\n",
    "tokenizer, model = get_tokenizer_and_model()\n",
    "\n",
    "# Your code to use the model and tokenizer here\n",
    "# e.g., model.generate(...) or tokenizer(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24cc3203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:13:08.749049Z",
     "iopub.status.busy": "2024-07-09T08:13:08.748668Z",
     "iopub.status.idle": "2024-07-09T08:13:10.061951Z",
     "shell.execute_reply": "2024-07-09T08:13:10.061034Z",
     "shell.execute_reply.started": "2024-07-09T08:13:08.749021Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-09 08:13:09--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-09 08:13:09 (68.7 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b72fcc5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:13:10.065570Z",
     "iopub.status.busy": "2024-07-09T08:13:10.065202Z",
     "iopub.status.idle": "2024-07-09T08:13:10.370146Z",
     "shell.execute_reply": "2024-07-09T08:13:10.369367Z",
     "shell.execute_reply.started": "2024-07-09T08:13:10.065541Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7efb877c2a00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86f98bef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:13:10.757346Z",
     "iopub.status.busy": "2024-07-09T08:13:10.756976Z",
     "iopub.status.idle": "2024-07-09T08:13:10.761666Z",
     "shell.execute_reply": "2024-07-09T08:13:10.760806Z",
     "shell.execute_reply.started": "2024-07-09T08:13:10.757318Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ebeb790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:13:13.409927Z",
     "iopub.status.busy": "2024-07-09T08:13:13.409518Z",
     "iopub.status.idle": "2024-07-09T08:13:13.414031Z",
     "shell.execute_reply": "2024-07-09T08:13:13.413230Z",
     "shell.execute_reply.started": "2024-07-09T08:13:13.409899Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0539c42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:13:13.528221Z",
     "iopub.status.busy": "2024-07-09T08:13:13.527826Z",
     "iopub.status.idle": "2024-07-09T08:13:13.533818Z",
     "shell.execute_reply": "2024-07-09T08:13:13.532914Z",
     "shell.execute_reply.started": "2024-07-09T08:13:13.528197Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, )\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc52f5eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:13:13.649227Z",
     "iopub.status.busy": "2024-07-09T08:13:13.648850Z",
     "iopub.status.idle": "2024-07-09T08:13:13.655653Z",
     "shell.execute_reply": "2024-07-09T08:13:13.654996Z",
     "shell.execute_reply.started": "2024-07-09T08:13:13.649202Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm(prompt, generate_params=None):\n",
    "    if generate_params is None:\n",
    "        generate_params = {}\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=generate_params.get(\"max_length\", 100),\n",
    "        num_beams=generate_params.get(\"num_beams\", 5),\n",
    "        do_sample=generate_params.get(\"do_sample\", False),\n",
    "        temperature=generate_params.get(\"temperature\", 1.0),\n",
    "        top_k=generate_params.get(\"top_k\", 50),\n",
    "        top_p=generate_params.get(\"top_p\", 0.95),\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56f36d35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T08:14:33.429583Z",
     "iopub.status.busy": "2024-07-09T08:14:33.428952Z",
     "iopub.status.idle": "2024-07-09T08:15:50.030260Z",
     "shell.execute_reply": "2024-07-09T08:15:50.029377Z",
     "shell.execute_reply.started": "2024-07-09T08:14:33.429547Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"I just discovered the course. Can I still join it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a1759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
